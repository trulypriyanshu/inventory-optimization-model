{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Online Retail Inventory Optimization System\n",
    "\n",
    "**Author:** Priyanshu Bhardwaj  \n",
    "**Date:** February 13, 2026  \n",
    "**GitHub:** [trulypriyanshu/online-retail-inventory-optimization](https://github.com/trulypriyanshu/online-retail-inventory-optimization)\n",
    "\n",
    "## üéØ Business Impact\n",
    "- Reduced excess inventory\n",
    "- Reduced Stockouts\n",
    "- Saved inventory costs\n",
    "- Improved service levels\n",
    "- Automated inventory optimization\n",
    "\n",
    "## üîç Features\n",
    "1. Comprehensive data cleaning and preprocessing\n",
    "2. Statistical demand forecasting\n",
    "3. ABC classification (Pareto analysis)\n",
    "4. Obsolescence risk detection\n",
    "5. Safety stock optimization\n",
    "6. Inventory performance metrics\n",
    "7. Visual analytics dashboard\n",
    "8. Export to Excel and Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"‚úÖ Output directory created: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Cleaning\n",
    "\n",
    "Load and clean the Online Retail dataset with proper data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(filepath='data/Online Retail.xlsx'):\n",
    "    \"\"\"\n",
    "    Load and clean the Online Retail dataset with proper data validation\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìÇ LOADING AND CLEANING DATA\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(f\"Loading data from: {filepath}\")\n",
    "        df = pd.read_excel(filepath)\n",
    "        \n",
    "        print(f\"‚úì Original dataset shape: {df.shape}\")\n",
    "        print(f\"‚úì Columns: {df.columns.tolist()}\")\n",
    "        print(f\"‚úì Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Standardize column names\n",
    "        df.columns = [col.strip().replace(' ', '_') for col in df.columns]\n",
    "        \n",
    "        # Store original stats\n",
    "        original_rows = len(df)\n",
    "        original_skus = df['StockCode'].nunique()\n",
    "        \n",
    "        # Create a clean copy\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # ========== DATA CLEANING STEPS ==========\n",
    "        \n",
    "        # 1. Remove cancelled orders\n",
    "        mask_cancelled = df_clean['InvoiceNo'].astype(str).str.startswith('C')\n",
    "        cancelled_count = mask_cancelled.sum()\n",
    "        df_clean = df_clean[~mask_cancelled]\n",
    "        print(f\"  Removed {cancelled_count:,} cancelled orders\")\n",
    "        \n",
    "        # 2. Remove negative quantities and prices\n",
    "        mask_negative_qty = df_clean['Quantity'] <= 0\n",
    "        mask_negative_price = df_clean['UnitPrice'] <= 0\n",
    "        negative_count = mask_negative_qty.sum() + mask_negative_price.sum()\n",
    "        df_clean = df_clean[(df_clean['Quantity'] > 0) & (df_clean['UnitPrice'] > 0)]\n",
    "        print(f\"  Removed {negative_count:,} rows with negative quantities/prices\")\n",
    "        \n",
    "        # 3. Remove missing CustomerID\n",
    "        missing_customer_before = df_clean['CustomerID'].isna().sum()\n",
    "        df_clean = df_clean.dropna(subset=['CustomerID'])\n",
    "        print(f\"  Removed {missing_customer_before:,} rows with missing CustomerID\")\n",
    "        \n",
    "        # 4. Remove extreme outliers (99th percentile)\n",
    "        q99_quantity = df_clean['Quantity'].quantile(0.99)\n",
    "        mask_outliers = df_clean['Quantity'] > q99_quantity\n",
    "        outlier_count = mask_outliers.sum()\n",
    "        df_clean = df_clean[~mask_outliers]\n",
    "        print(f\"  Removed {outlier_count:,} extreme quantity outliers (> {q99_quantity:.0f} units)\")\n",
    "        \n",
    "        # ========== FEATURE ENGINEERING ==========\n",
    "        \n",
    "        # Convert InvoiceDate to datetime\n",
    "        df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])\n",
    "        \n",
    "        # Create essential features\n",
    "        df_clean['TotalValue'] = df_clean['Quantity'] * df_clean['UnitPrice']\n",
    "        df_clean['Date'] = df_clean['InvoiceDate'].dt.date\n",
    "        df_clean['YearMonth'] = df_clean['InvoiceDate'].dt.to_period('M')\n",
    "        df_clean['DayOfWeek'] = df_clean['InvoiceDate'].dt.day_name()\n",
    "        df_clean['Month'] = df_clean['InvoiceDate'].dt.month\n",
    "        df_clean['Year'] = df_clean['InvoiceDate'].dt.year\n",
    "        \n",
    "        # ========== FINAL STATISTICS ==========\n",
    "        \n",
    "        cleaned_rows = len(df_clean)\n",
    "        cleaned_skus = df_clean['StockCode'].nunique()\n",
    "        data_retention = (cleaned_rows / original_rows) * 100\n",
    "        \n",
    "        print(\"\\nüìä CLEANING SUMMARY:\")\n",
    "        print(f\"  Original rows: {original_rows:,}\")\n",
    "        print(f\"  Cleaned rows: {cleaned_rows:,}\")\n",
    "        print(f\"  Data retention: {data_retention:.1f}%\")\n",
    "        print(f\"  Original SKUs: {original_skus:,}\")\n",
    "        print(f\"  Cleaned SKUs: {cleaned_skus:,}\")\n",
    "        print(f\"  Time range: {df_clean['InvoiceDate'].min().date()} to {df_clean['InvoiceDate'].max().date()}\")\n",
    "        print(f\"  Total revenue: ${df_clean['TotalValue'].sum():,.2f}\")\n",
    "        print(f\"  Avg transaction value: ${df_clean['TotalValue'].mean():.2f}\")\n",
    "        \n",
    "        # Create summary dictionary\n",
    "        summary_stats = {\n",
    "            'original_rows': original_rows,\n",
    "            'cleaned_rows': cleaned_rows,\n",
    "            'data_retention_pct': data_retention,\n",
    "            'original_skus': original_skus,\n",
    "            'cleaned_skus': cleaned_skus,\n",
    "            'total_revenue': df_clean['TotalValue'].sum(),\n",
    "            'time_range_start': df_clean['InvoiceDate'].min(),\n",
    "            'time_range_end': df_clean['InvoiceDate'].max()\n",
    "        }\n",
    "        \n",
    "        return df_clean, summary_stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Execute data loading\n",
    "df_clean, summary_stats = load_and_clean_data('data/Online Retail.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Demand Analysis\n",
    "\n",
    "Analyze demand patterns for each SKU with statistical rigor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sku_demand(df_clean, min_days_with_sales=30):\n",
    "    \"\"\"\n",
    "    Analyze demand patterns for each SKU with statistical rigor\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìà ANALYZING SKU DEMAND PATTERNS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create daily demand dataset\n",
    "    print(\"Creating daily demand aggregation...\")\n",
    "    daily_demand = df_clean.groupby(['StockCode', 'Date']).agg({\n",
    "        'Quantity': 'sum',\n",
    "        'TotalValue': 'sum',\n",
    "        'UnitPrice': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate comprehensive demand statistics per SKU\n",
    "    print(\"Calculating SKU-level demand statistics...\")\n",
    "    \n",
    "    sku_stats = daily_demand.groupby('StockCode').agg({\n",
    "        'Quantity': ['mean', 'std', 'count', 'min', 'max', 'median'],\n",
    "        'TotalValue': 'sum',\n",
    "        'UnitPrice': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    sku_stats.columns = [\n",
    "        'StockCode', \n",
    "        'AvgDailyDemand', 'DemandStd', 'DaysWithSales', \n",
    "        'MinDailyDemand', 'MaxDailyDemand', 'MedianDailyDemand',\n",
    "        'TotalRevenue', 'AvgUnitPrice'\n",
    "    ]\n",
    "    \n",
    "    # Add product description\n",
    "    product_info = (\n",
    "        df_clean.groupby('StockCode')['Description']\n",
    "        .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "        .reset_index()\n",
    "    )\n",
    "    sku_stats = sku_stats.merge(product_info, on='StockCode', how='left')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    sku_stats['DemandCV'] = sku_stats['DemandStd'] / sku_stats['AvgDailyDemand']\n",
    "    \n",
    "    # Handle zero/negative demand and NaN values\n",
    "    sku_stats['AvgDailyDemand'] = sku_stats['AvgDailyDemand'].clip(lower=0.1)\n",
    "    sku_stats['DemandStd'] = sku_stats['DemandStd'].fillna(sku_stats['AvgDailyDemand'] * 0.3)\n",
    "    sku_stats['DemandCV'] = sku_stats['DemandCV'].fillna(0.5).clip(upper=3.0)\n",
    "    \n",
    "    # Calculate demand patterns\n",
    "    sku_stats['DemandStability'] = np.where(\n",
    "        sku_stats['DemandCV'] < 0.5, 'Stable',\n",
    "        np.where(sku_stats['DemandCV'] < 1.0, 'Moderate', 'Volatile')\n",
    "    )\n",
    "    \n",
    "    # Filter SKUs with sufficient data\n",
    "    sufficient_mask = sku_stats['DaysWithSales'] >= min_days_with_sales\n",
    "    sufficient_skus = sku_stats[sufficient_mask].copy()\n",
    "    insufficient_skus = sku_stats[~sufficient_mask].copy()\n",
    "    \n",
    "    print(f\"\\nüìä DEMAND ANALYSIS RESULTS:\")\n",
    "    print(f\"  Total SKUs analyzed: {len(sku_stats):,}\")\n",
    "    print(f\"  SKUs with ‚â•{min_days_with_sales} days sales: {len(sufficient_skus):,} ({len(sufficient_skus)/len(sku_stats)*100:.1f}%)\")\n",
    "    print(f\"  SKUs with insufficient history: {len(insufficient_skus):,}\")\n",
    "    \n",
    "    # Demand variability summary\n",
    "    print(f\"\\n  Demand Variability Distribution:\")\n",
    "    for stability in ['Stable', 'Moderate', 'Volatile']:\n",
    "        count = (sufficient_skus['DemandStability'] == stability).sum()\n",
    "        pct = count / len(sufficient_skus) * 100\n",
    "        print(f\"    {stability}: {count:,} SKUs ({pct:.1f}%)\")\n",
    "    \n",
    "    # Top SKUs by revenue\n",
    "    print(f\"\\n  Top 5 SKUs by Revenue:\")\n",
    "    top_skus = sufficient_skus.nlargest(5, 'TotalRevenue')[['StockCode', 'Description', 'TotalRevenue', 'AvgDailyDemand']]\n",
    "    for idx, row in top_skus.iterrows():\n",
    "        print(f\"    {row['StockCode']}: ${row['TotalRevenue']:,.2f} ({row['AvgDailyDemand']:.1f} units/day)\")\n",
    "    \n",
    "    return sufficient_skus, daily_demand\n",
    "\n",
    "# Execute demand analysis\n",
    "sku_stats, daily_demand = analyze_sku_demand(df_clean, min_days_with_sales=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obsolescence Risk Detection\n",
    "\n",
    "Detect obsolescence risk from product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_obsolescence_risk(description):\n",
    "    \"\"\"\n",
    "    Detect obsolescence risk from product description with detailed categorization\n",
    "    \"\"\"\n",
    "    if pd.isna(description):\n",
    "        return {'risk_level': 'Medium', 'risk_type': 'Unknown', 'confidence': 0.5}\n",
    "    \n",
    "    desc_upper = str(description).upper()\n",
    "    \n",
    "    # Define risk patterns\n",
    "    seasonal_terms = {\n",
    "        'CHRISTMAS': 0.9, 'XMAS': 0.9, 'NOEL': 0.8,\n",
    "        'EASTER': 0.8, 'EGG': 0.7,\n",
    "        'HALLOWEEN': 0.8, 'PUMPKIN': 0.7,\n",
    "        'SUMMER': 0.6, 'WINTER': 0.6, 'SPRING': 0.6, 'AUTUMN': 0.6,\n",
    "        'SEASONAL': 0.8, 'FESTIVE': 0.7, 'HOLIDAY': 0.7,\n",
    "        'DECORATION': 0.5\n",
    "    }\n",
    "    \n",
    "    fashion_terms = {\n",
    "        'RETRO': 0.7, 'VINTAGE': 0.6, 'TRENDY': 0.8,\n",
    "        'FASHION': 0.7, 'STYLE': 0.5, 'DESIGNER': 0.6,\n",
    "        'COLLECTION': 0.5, 'MODERN': 0.5, 'CONTEMPORARY': 0.5\n",
    "    }\n",
    "    \n",
    "    perishable_terms = {\n",
    "        'FOOD': 0.9, 'CHOCOLATE': 0.8, 'PERISHABLE': 0.9,\n",
    "        'EXPIR': 0.9, 'FRESH': 0.7, 'BAKED': 0.7\n",
    "    }\n",
    "    \n",
    "    # Calculate risk scores\n",
    "    seasonal_score = sum(score for term, score in seasonal_terms.items() if term in desc_upper)\n",
    "    fashion_score = sum(score for term, score in fashion_terms.items() if term in desc_upper)\n",
    "    perishable_score = sum(score for term, score in perishable_terms.items() if term in desc_upper)\n",
    "    \n",
    "    # Determine primary risk type and level\n",
    "    max_score = max(seasonal_score, fashion_score, perishable_score)\n",
    "    \n",
    "    if max_score >= 0.8:\n",
    "        risk_level = 'High'\n",
    "        confidence = min(1.0, max_score)\n",
    "    elif max_score >= 0.5:\n",
    "        risk_level = 'Medium'\n",
    "        confidence = max_score\n",
    "    else:\n",
    "        risk_level = 'Low'\n",
    "        confidence = 0.3\n",
    "    \n",
    "    # Determine risk type\n",
    "    if seasonal_score == max_score and seasonal_score > 0:\n",
    "        risk_type = 'Seasonal'\n",
    "    elif fashion_score == max_score and fashion_score > 0:\n",
    "        risk_type = 'Fashion'\n",
    "    elif perishable_score == max_score and perishable_score > 0:\n",
    "        risk_type = 'Perishable'\n",
    "    else:\n",
    "        risk_type = 'Stable'\n",
    "        confidence = 0.2\n",
    "    \n",
    "    return {\n",
    "        'risk_level': risk_level,\n",
    "        'risk_type': risk_type,\n",
    "        'confidence': round(confidence, 2)\n",
    "    }\n",
    "\n",
    "def add_obsolescence_features(sku_stats):\n",
    "    \"\"\"\n",
    "    Add obsolescence risk features to SKU statistics\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç ANALYZING OBSOLESCENCE RISKS\")\n",
    "    \n",
    "    # Apply obsolescence detection\n",
    "    risk_results = sku_stats['Description'].apply(detect_obsolescence_risk)\n",
    "    \n",
    "    # Extract to columns\n",
    "    sku_stats['ObsolescenceRisk'] = [r['risk_level'] for r in risk_results]\n",
    "    sku_stats['ObsolescenceType'] = [r['risk_type'] for r in risk_results]\n",
    "    sku_stats['RiskConfidence'] = [r['confidence'] for r in risk_results]\n",
    "    \n",
    "    # Print summary\n",
    "    risk_counts = sku_stats['ObsolescenceRisk'].value_counts()\n",
    "    print(f\"  Obsolescence Risk Distribution:\")\n",
    "    for risk_level in ['High', 'Medium', 'Low']:\n",
    "        count = risk_counts.get(risk_level, 0)\n",
    "        pct = count / len(sku_stats) * 100\n",
    "        print(f\"    {risk_level}: {count:,} SKUs ({pct:.1f}%)\")\n",
    "    \n",
    "    return sku_stats\n",
    "\n",
    "# Execute obsolescence analysis\n",
    "sku_stats = add_obsolescence_features(sku_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ABC Analysis (Pareto Analysis)\n",
    "\n",
    "Classify SKUs based on revenue contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_abc_analysis(sku_stats, percentiles=(80, 95)):\n",
    "    \"\"\"\n",
    "    Perform ABC analysis with configurable percentiles\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üè∑Ô∏è PERFORMING ABC ANALYSIS (PARETO ANALYSIS)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    a_threshold, b_threshold = percentiles\n",
    "    \n",
    "    # Sort by revenue descending\n",
    "    sku_sorted = sku_stats.sort_values('TotalRevenue', ascending=False).copy()\n",
    "    \n",
    "    # Calculate cumulative percentages\n",
    "    sku_sorted['CumulativeRevenue'] = sku_sorted['TotalRevenue'].cumsum()\n",
    "    sku_sorted['CumulativePct'] = (sku_sorted['CumulativeRevenue'] / \n",
    "                                   sku_sorted['TotalRevenue'].sum() * 100)\n",
    "    \n",
    "    # Assign ABC class based on cumulative percentage\n",
    "    def assign_abc_class(cum_pct):\n",
    "        if cum_pct <= a_threshold:\n",
    "            return 'A'\n",
    "        elif cum_pct <= b_threshold:\n",
    "            return 'B'\n",
    "        else:\n",
    "            return 'C'\n",
    "    \n",
    "    sku_sorted['ABC_Class'] = sku_sorted['CumulativePct'].apply(assign_abc_class)\n",
    "    \n",
    "    # Calculate statistics per ABC class\n",
    "    abc_summary = sku_sorted.groupby('ABC_Class').agg({\n",
    "        'StockCode': 'count',\n",
    "        'TotalRevenue': 'sum',\n",
    "        'AvgDailyDemand': 'mean',\n",
    "        'AvgUnitPrice': 'mean',\n",
    "        'DaysWithSales': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    abc_summary['Pct_SKUs'] = (abc_summary['StockCode'] / abc_summary['StockCode'].sum()) * 100\n",
    "    abc_summary['Pct_Revenue'] = (abc_summary['TotalRevenue'] / abc_summary['TotalRevenue'].sum()) * 100\n",
    "    abc_summary['Revenue_per_SKU'] = abc_summary['TotalRevenue'] / abc_summary['StockCode']\n",
    "    \n",
    "    print(f\"\\nüìä ABC ANALYSIS RESULTS ({a_threshold}/{b_threshold} Percentiles):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Class':<6} {'SKUs':>8} {'% SKUs':>8} {'Revenue':>12} {'% Revenue':>10} {'Rev/SKU':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for _, row in abc_summary.iterrows():\n",
    "        print(f\"{row['ABC_Class']:<6} \"\n",
    "              f\"{row['StockCode']:>8,} \"\n",
    "              f\"{row['Pct_SKUs']:>7.1f}% \"\n",
    "              f\"${row['TotalRevenue']:>11,.0f} \"\n",
    "              f\"{row['Pct_Revenue']:>9.1f}% \"\n",
    "              f\"${row['Revenue_per_SKU']:>11,.0f}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return sku_sorted, abc_summary\n",
    "\n",
    "# Execute ABC analysis\n",
    "sku_stats, abc_summary = perform_abc_analysis(sku_stats, percentiles=(80, 95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lead Time Assignment\n",
    "\n",
    "Assign realistic lead times based on ABC class and other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_lead_times(sku_stats):\n",
    "    \"\"\"\n",
    "    Assign realistic lead times based on ABC class, obsolescence risk, and other factors\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚è±Ô∏è ASSIGNING LEAD TIMES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Base lead times by ABC class\n",
    "    base_lead_times = {\n",
    "        'A': {'min': 3, 'max': 7, 'mean': 5},\n",
    "        'B': {'min': 7, 'max': 14, 'mean': 10},\n",
    "        'C': {'min': 14, 'max': 30, 'mean': 21}\n",
    "    }\n",
    "    \n",
    "    def calculate_lead_time(row):\n",
    "        # Get base lead time based on ABC class\n",
    "        base = base_lead_times[row['ABC_Class']]\n",
    "        \n",
    "        # Start with base mean\n",
    "        lead_time = base['mean']\n",
    "        \n",
    "        # Adjust for obsolescence risk\n",
    "        if row['ObsolescenceRisk'] == 'High':\n",
    "            lead_time = max(base['min'], lead_time * 0.7)\n",
    "        elif row['ObsolescenceRisk'] == 'Low':\n",
    "            lead_time = min(base['max'], lead_time * 1.2)\n",
    "        \n",
    "        # Adjust for unit price\n",
    "        if row['AvgUnitPrice'] > 100:\n",
    "            lead_time *= 1.3\n",
    "        elif row['AvgUnitPrice'] < 10:\n",
    "            lead_time *= 0.8\n",
    "        \n",
    "        # Adjust for demand stability\n",
    "        if row['DemandStability'] == 'Volatile':\n",
    "            lead_time *= 0.8\n",
    "        elif row['DemandStability'] == 'Stable':\n",
    "            lead_time *= 1.2\n",
    "        \n",
    "        # Round and ensure within bounds\n",
    "        lead_time = round(lead_time)\n",
    "        lead_time = max(base['min'], min(base['max'], lead_time))\n",
    "        \n",
    "        return lead_time\n",
    "    \n",
    "    def calculate_lead_time_std(row):\n",
    "        \"\"\"Calculate lead time standard deviation\"\"\"\n",
    "        base_std = row['LeadTimeDays'] * 0.2\n",
    "        \n",
    "        if row['ObsolescenceRisk'] == 'High':\n",
    "            base_std *= 1.5\n",
    "        \n",
    "        desc = str(row['Description']).upper()\n",
    "        if any(term in desc for term in ['IMPORT', 'CHINA', 'ASIA', 'EUROPE', 'ITALY']):\n",
    "            base_std *= 1.8\n",
    "        \n",
    "        return round(base_std, 1)\n",
    "    \n",
    "    # Apply lead time calculations\n",
    "    sku_stats['LeadTimeDays'] = sku_stats.apply(calculate_lead_time, axis=1)\n",
    "    sku_stats['LeadTimeStd'] = sku_stats.apply(calculate_lead_time_std, axis=1)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìä LEAD TIME DISTRIBUTION:\")\n",
    "    for abc_class in ['A', 'B', 'C']:\n",
    "        class_data = sku_stats[sku_stats['ABC_Class'] == abc_class]\n",
    "        if len(class_data) > 0:\n",
    "            mean_lt = class_data['LeadTimeDays'].mean()\n",
    "            std_lt = class_data['LeadTimeStd'].mean()\n",
    "            print(f\"  {abc_class}-items: {mean_lt:.1f} ¬± {std_lt:.1f} days \"\n",
    "                  f\"(range: {class_data['LeadTimeDays'].min()}-{class_data['LeadTimeDays'].max()})\")\n",
    "    \n",
    "    return sku_stats\n",
    "\n",
    "# Execute lead time assignment\n",
    "sku_stats = assign_lead_times(sku_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Service Level Calculations\n",
    "\n",
    "Calculate target and achievable service levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_service_level_target(row):\n",
    "    \"\"\"\n",
    "    Calculate appropriate service level target based on ABC class and obsolescence risk\n",
    "    \"\"\"\n",
    "    # Base service levels by ABC class\n",
    "    base_service = {\n",
    "        'A': 0.97,\n",
    "        'B': 0.95,\n",
    "        'C': 0.90\n",
    "    }\n",
    "    \n",
    "    target = base_service.get(row['ABC_Class'], 0.95)\n",
    "    \n",
    "    # Adjust for obsolescence risk\n",
    "    if row['ObsolescenceRisk'] == 'High':\n",
    "        target = max(0.85, target - 0.05)\n",
    "    elif row['ObsolescenceRisk'] == 'Low':\n",
    "        target = min(0.99, target + 0.02)\n",
    "    \n",
    "    return round(target, 3)\n",
    "\n",
    "def calculate_achievable_service_level(row):\n",
    "    \"\"\"\n",
    "    Calculate what service level current stock can actually provide\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Mean and std of demand during lead time\n",
    "    mean_demand_lt = row['AvgDailyDemand'] * row['LeadTimeDays']\n",
    "    std_demand_lt = np.sqrt(\n",
    "        row['LeadTimeDays'] * (row['DemandStd'] ** 2) + \n",
    "        (row['AvgDailyDemand'] ** 2) * (row['LeadTimeStd'] ** 2)\n",
    "    )\n",
    "    \n",
    "    if std_demand_lt == 0 or mean_demand_lt == 0:\n",
    "        return 0.5\n",
    "    \n",
    "    # Z-score for current stock position\n",
    "    z_actual = (row['CurrentStock'] - mean_demand_lt) / std_demand_lt\n",
    "    \n",
    "    # Service level = Probability(demand ‚â§ current stock)\n",
    "    service_level = stats.norm.cdf(z_actual)\n",
    "    \n",
    "    return round(max(0, min(1, service_level)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Safety Stock & Reorder Point Calculation\n",
    "\n",
    "Calculate optimal safety stock and reorder points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_safety_stock(row):\n",
    "    \"\"\"\n",
    "    Calculate safety stock using statistical formula with service level target\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Get service level target\n",
    "    service_level = row['ServiceLevelTarget']\n",
    "    \n",
    "    # Get Z-score for service level\n",
    "    z_score = stats.norm.ppf(service_level)\n",
    "    \n",
    "    # Extract components\n",
    "    D = row['AvgDailyDemand']\n",
    "    œÉ_D = max(row['DemandStd'], D * 0.1)\n",
    "    L = row['LeadTimeDays']\n",
    "    œÉ_L = row['LeadTimeStd']\n",
    "    \n",
    "    # Safety stock calculation\n",
    "    ss = z_score * np.sqrt(L * (œÉ_D ** 2) + (D ** 2) * (œÉ_L ** 2))\n",
    "    \n",
    "    # Apply minimum safety stock\n",
    "    min_ss = D * 0.5\n",
    "    \n",
    "    return round(max(min_ss, ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stock Status Determination\n",
    "\n",
    "Determine if SKUs are optimally stocked, overstocked, or understocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_stock_status(row):\n",
    "    \"\"\"\n",
    "    Determine stock status with CORRECT LOGIC (no double-counting safety stock)\n",
    "    \"\"\"\n",
    "    # Calculate key metrics\n",
    "    days_on_hand = row['CurrentStock'] / max(row['AvgDailyDemand'], 0.1)\n",
    "    \n",
    "    # Define business rules by ABC class\n",
    "    business_rules = {\n",
    "        'A': {\n",
    "            'max_days': 30,\n",
    "            'tolerance_pct': 1.10,\n",
    "            'reason': 'High-value items need tight control'\n",
    "        },\n",
    "        'B': {\n",
    "            'max_days': 45,\n",
    "            'tolerance_pct': 1.20,\n",
    "            'reason': 'Balanced approach for moderate-value items'\n",
    "        },\n",
    "        'C': {\n",
    "            'max_days': 60,\n",
    "            'tolerance_pct': 1.30,\n",
    "            'reason': 'Low-value items, ordering efficiency matters'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    rules = business_rules.get(row['ABC_Class'], business_rules['C'])\n",
    "    \n",
    "    # Calculate tolerance threshold\n",
    "    tolerance_threshold = row['ReorderPoint'] * rules['tolerance_pct']\n",
    "    \n",
    "    # Determine status\n",
    "    status_reasons = []\n",
    "    \n",
    "    # Check for UNDERSTOCKED\n",
    "    if row['CurrentStock'] < row['ReorderPoint']:\n",
    "        shortage = row['ReorderPoint'] - row['CurrentStock']\n",
    "        status_reasons.append(f\"Understocked by {int(shortage)} units\")\n",
    "        return 'Understocked', \"; \".join(status_reasons)\n",
    "    \n",
    "    # Check for OVERSTOCKED\n",
    "    overstock_conditions = []\n",
    "    \n",
    "    if days_on_hand > rules['max_days']:\n",
    "        overstock_conditions.append(f\"{days_on_hand:.1f} days inventory > {rules['max_days']} day limit\")\n",
    "    \n",
    "    if row['CurrentStock'] > tolerance_threshold:\n",
    "        overstock_conditions.append(f\"Stock {row['CurrentStock']} > tolerance {tolerance_threshold:.0f}\")\n",
    "    \n",
    "    if overstock_conditions:\n",
    "        status_reasons.append(f\"Overstocked: {', '.join(overstock_conditions)}\")\n",
    "        return 'Overstocked', \"; \".join(status_reasons)\n",
    "    \n",
    "    # Otherwise OPTIMAL\n",
    "    status_reasons.append(f\"Within {row['ABC_Class']}-item guidelines\")\n",
    "    return 'Optimal', \"; \".join(status_reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Inventory Metrics\n",
    "\n",
    "Calculate all inventory metrics and simulate current stock levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inventory_metrics(sku_stats):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive inventory metrics for each SKU\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üì¶ CALCULATING INVENTORY METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. Calculate service level targets\n",
    "    print(\"Calculating service level targets...\")\n",
    "    sku_stats['ServiceLevelTarget'] = sku_stats.apply(calculate_service_level_target, axis=1)\n",
    "    \n",
    "    # 2. Calculate safety stock\n",
    "    print(\"Calculating safety stock...\")\n",
    "    sku_stats['SafetyStock'] = sku_stats.apply(calculate_safety_stock, axis=1)\n",
    "    \n",
    "    print(\"Calculating reorder points...\")\n",
    "    sku_stats['ReorderPoint'] = (sku_stats['AvgDailyDemand'] * sku_stats['LeadTimeDays'] + \n",
    "                                sku_stats['SafetyStock']).round(0)\n",
    "    \n",
    "    # 3. Simulate current stock\n",
    "    print(\"Simulating current stock levels...\")\n",
    "    np.random.seed(42)\n",
    "    sku_stats['CurrentStock'] = (sku_stats['ReorderPoint'] * \n",
    "                                np.random.uniform(0.5, 2.0, len(sku_stats))).round(0)\n",
    "    \n",
    "    # 4. Calculate achievable service level\n",
    "    print(\"Calculating achievable service levels...\")\n",
    "    sku_stats['ServiceLevelAchievable'] = sku_stats.apply(calculate_achievable_service_level, axis=1)\n",
    "    \n",
    "    # 5. Determine stock status\n",
    "    print(\"Determining stock status...\")\n",
    "    status_results = sku_stats.apply(\n",
    "        lambda row: pd.Series(determine_stock_status(row)), \n",
    "        axis=1\n",
    "    )\n",
    "    sku_stats['StockStatus'] = status_results[0]\n",
    "    sku_stats['StatusReason'] = status_results[1]\n",
    "    \n",
    "    # 6. Calculate excess/deficit stock\n",
    "    print(\"Calculating excess/deficit quantities...\")\n",
    "    \n",
    "    def calculate_excess_deficit(row):\n",
    "        if row['StockStatus'] == 'Overstocked':\n",
    "            rules = {'A': 1.10, 'B': 1.20, 'C': 1.30}\n",
    "            tolerance = rules.get(row['ABC_Class'], 1.20)\n",
    "            threshold = row['ReorderPoint'] * tolerance\n",
    "            excess = max(0, row['CurrentStock'] - threshold)\n",
    "            return round(excess), 0\n",
    "        elif row['StockStatus'] == 'Understocked':\n",
    "            deficit = max(0, row['ReorderPoint'] - row['CurrentStock'])\n",
    "            return 0, round(deficit)\n",
    "        else:\n",
    "            return 0, 0\n",
    "    \n",
    "    excess_deficit = sku_stats.apply(\n",
    "        lambda row: pd.Series(calculate_excess_deficit(row)), \n",
    "        axis=1\n",
    "    )\n",
    "    sku_stats['ExcessStock'] = excess_deficit[0]\n",
    "    sku_stats['DeficitStock'] = excess_deficit[1]\n",
    "    \n",
    "    # 7. Calculate inventory value and costs\n",
    "    print(\"Calculating inventory values and costs...\")\n",
    "    sku_stats['InventoryValue'] = (sku_stats['CurrentStock'] * sku_stats['AvgUnitPrice']).round(2)\n",
    "    sku_stats['ExcessValue'] = (sku_stats['ExcessStock'] * sku_stats['AvgUnitPrice']).round(2)\n",
    "    sku_stats['DeficitValue'] = (sku_stats['DeficitStock'] * sku_stats['AvgUnitPrice']).round(2)\n",
    "    \n",
    "    # Calculate holding costs (25% annual carrying cost)\n",
    "    daily_holding_rate = 0.25 / 365\n",
    "    sku_stats['DailyHoldingCost'] = (sku_stats['InventoryValue'] * daily_holding_rate).round(2)\n",
    "    sku_stats['ExcessHoldingCost'] = (sku_stats['ExcessValue'] * daily_holding_rate).round(2)\n",
    "    \n",
    "    # 8. Calculate key performance metrics\n",
    "    sku_stats['StockoutRisk'] = (1 - sku_stats['ServiceLevelAchievable']).round(3)\n",
    "    sku_stats['ServiceLevelGap'] = (sku_stats['ServiceLevelTarget'] - \n",
    "                                   sku_stats['ServiceLevelAchievable']).round(3)\n",
    "    \n",
    "    # Days of inventory\n",
    "    sku_stats['DaysOnHand'] = (sku_stats['CurrentStock'] / \n",
    "                              sku_stats['AvgDailyDemand'].clip(lower=0.1)).round(1)\n",
    "    \n",
    "    # Inventory turns (annual)\n",
    "    sku_stats['AnnualTurns'] = (365 * sku_stats['AvgDailyDemand'] / \n",
    "                               sku_stats['CurrentStock'].clip(lower=1)).round(1)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nüìä INVENTORY METRICS SUMMARY:\")\n",
    "    print(f\"  Total SKUs analyzed: {len(sku_stats):,}\")\n",
    "    \n",
    "    status_counts = sku_stats['StockStatus'].value_counts()\n",
    "    for status in ['Optimal', 'Overstocked', 'Understocked']:\n",
    "        count = status_counts.get(status, 0)\n",
    "        pct = count / len(sku_stats) * 100\n",
    "        print(f\"  {status}: {count:,} SKUs ({pct:.1f}%)\")\n",
    "    \n",
    "    total_excess = sku_stats['ExcessValue'].sum()\n",
    "    total_deficit = sku_stats['DeficitValue'].sum()\n",
    "    total_inventory = sku_stats['InventoryValue'].sum()\n",
    "    \n",
    "    print(f\"\\n  Total Inventory Value: ${total_inventory:,.2f}\")\n",
    "    print(f\"  Total Excess Value: ${total_excess:,.2f}\")\n",
    "    print(f\"  Total Deficit Value: ${total_deficit:,.2f}\")\n",
    "    print(f\"  Excess % of Total: {total_excess/total_inventory*100:.1f}%\")\n",
    "    \n",
    "    # Calculate potential savings\n",
    "    annual_excess_cost = sku_stats['ExcessHoldingCost'].sum() * 365\n",
    "    print(f\"\\nüí∞ POTENTIAL ANNUAL SAVINGS:\")\n",
    "    print(f\"  From reducing excess inventory: ${annual_excess_cost:,.2f}\")\n",
    "    \n",
    "    return sku_stats\n",
    "\n",
    "# Execute inventory metrics calculation\n",
    "sku_stats = calculate_inventory_metrics(sku_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Visualization\n",
    "\n",
    "Create comprehensive visualizations of the inventory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(sku_stats, abc_summary):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for the analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä CREATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. PARETO CHART\n",
    "    print(\"Creating Pareto chart...\")\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    \n",
    "    pareto_data = sku_stats.sort_values('TotalRevenue', ascending=False).copy()\n",
    "    pareto_data['CumulativePct'] = (pareto_data['TotalRevenue'].cumsum() / \n",
    "                                   pareto_data['TotalRevenue'].sum() * 100)\n",
    "    pareto_data['ItemNumber'] = range(1, len(pareto_data) + 1)\n",
    "    \n",
    "    show_top = min(100, len(pareto_data))\n",
    "    bars = ax1.bar(pareto_data['ItemNumber'].iloc[:show_top], \n",
    "                   pareto_data['TotalRevenue'].iloc[:show_top], \n",
    "                   color='skyblue', alpha=0.7)\n",
    "    ax1.set_xlabel('SKU Rank (by Revenue)', fontsize=10)\n",
    "    ax1.set_ylabel('Revenue ($)', color='blue', fontsize=10)\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_title('Pareto Chart: Revenue Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax1_twin = ax1.twinx()\n",
    "    ax1_twin.plot(pareto_data['ItemNumber'].iloc[:show_top], \n",
    "                  pareto_data['CumulativePct'].iloc[:show_top], \n",
    "                  'r-', linewidth=2)\n",
    "    ax1_twin.set_ylabel('Cumulative %', color='red', fontsize=10)\n",
    "    ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "    ax1_twin.axhline(y=80, color='green', linestyle='--', alpha=0.7)\n",
    "    ax1_twin.axhline(y=95, color='orange', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 2. ABC CLASS DISTRIBUTION\n",
    "    print(\"Creating ABC distribution pie chart...\")\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    abc_counts = abc_summary.set_index('ABC_Class')['StockCode']\n",
    "    colors = {'A': '#FFD700', 'B': '#C0C0C0', 'C': '#CD7F32'}\n",
    "    \n",
    "    pie_colors = [colors.get(cls, '#999999') for cls in abc_counts.index]\n",
    "    wedges, texts, autotexts = ax2.pie(abc_counts.values, labels=abc_counts.index, \n",
    "                                       colors=pie_colors, autopct='%1.1f%%',\n",
    "                                       startangle=90)\n",
    "    ax2.set_title('ABC Class Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 3. STOCK STATUS BY ABC CLASS\n",
    "    print(\"Creating stock status by ABC class chart...\")\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    \n",
    "    status_by_abc = pd.crosstab(sku_stats['ABC_Class'], sku_stats['StockStatus'])\n",
    "    status_by_abc = status_by_abc.reindex(['A', 'B', 'C'])\n",
    "    status_by_abc = status_by_abc[['Understocked', 'Optimal', 'Overstocked']]\n",
    "    \n",
    "    colors_status = {'Understocked': '#FF6B6B', 'Optimal': '#4ECDC4', 'Overstocked': '#45B7D1'}\n",
    "    color_list = [colors_status[col] for col in status_by_abc.columns]\n",
    "    \n",
    "    status_by_abc.plot(kind='bar', stacked=True, ax=ax3, color=color_list, alpha=0.8)\n",
    "    ax3.set_xlabel('ABC Class', fontsize=10)\n",
    "    ax3.set_ylabel('Number of SKUs', fontsize=10)\n",
    "    ax3.set_title('Stock Status by ABC Class', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(title='Stock Status')\n",
    "    ax3.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # 4. SERVICE LEVEL ACHIEVEMENT\n",
    "    print(\"Creating service level analysis chart...\")\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    \n",
    "    sample_size = min(200, len(sku_stats))\n",
    "    sample_data = sku_stats.sample(sample_size, random_state=42)\n",
    "    \n",
    "    color_map = {'A': 0, 'B': 1, 'C': 2}\n",
    "    colors_abc = [color_map.get(cls, 0) for cls in sample_data['ABC_Class']]\n",
    "    \n",
    "    scatter = ax4.scatter(sample_data['ServiceLevelTarget'] * 100, \n",
    "                         sample_data['ServiceLevelAchievable'] * 100,\n",
    "                         c=colors_abc, cmap='viridis', s=50, alpha=0.6)\n",
    "    \n",
    "    ax4.plot([0, 100], [0, 100], 'r--', alpha=0.5)\n",
    "    ax4.set_xlabel('Target Service Level (%)', fontsize=10)\n",
    "    ax4.set_ylabel('Achievable Service Level (%)', fontsize=10)\n",
    "    ax4.set_title('Service Level: Target vs Achievable', fontsize=12, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_xlim(80, 100)\n",
    "    ax4.set_ylim(80, 100)\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('ABC Class (0=A, 1=B, 2=C)', fontsize=9)\n",
    "    \n",
    "    # 5. DEMAND VARIABILITY ANALYSIS\n",
    "    print(\"Creating demand variability chart...\")\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    \n",
    "    demand_summary = sku_stats.groupby(['DemandStability', 'ABC_Class']).size().unstack()\n",
    "    demand_summary = demand_summary.reindex(['Stable', 'Moderate', 'Volatile'])\n",
    "    demand_summary = demand_summary[['A', 'B', 'C']]\n",
    "    \n",
    "    demand_summary.plot(kind='bar', ax=ax5, color=['#FFD700', '#C0C0C0', '#CD7F32'], alpha=0.8)\n",
    "    ax5.set_xlabel('Demand Stability', fontsize=10)\n",
    "    ax5.set_ylabel('Number of SKUs', fontsize=10)\n",
    "    ax5.set_title('Demand Stability by ABC Class', fontsize=12, fontweight='bold')\n",
    "    ax5.legend(title='ABC Class')\n",
    "    ax5.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # 6. OBSOLESCENCE RISK ANALYSIS\n",
    "    print(\"Creating obsolescence risk chart...\")\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    \n",
    "    obsolescence_summary = sku_stats.groupby(['ObsolescenceRisk', 'ABC_Class']).size().unstack()\n",
    "    obsolescence_summary = obsolescence_summary.reindex(['High', 'Medium', 'Low'])\n",
    "    obsolescence_summary = obsolescence_summary[['A', 'B', 'C']]\n",
    "    \n",
    "    obsolescence_summary.plot(kind='bar', ax=ax6, color=['#FFD700', '#C0C0C0', '#CD7F32'], alpha=0.8)\n",
    "    ax6.set_xlabel('Obsolescence Risk', fontsize=10)\n",
    "    ax6.set_ylabel('Number of SKUs', fontsize=10)\n",
    "    ax6.set_title('Obsolescence Risk by ABC Class', fontsize=12, fontweight='bold')\n",
    "    ax6.legend(title='ABC Class')\n",
    "    ax6.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # 7. EXCESS INVENTORY VALUE\n",
    "    print(\"Creating excess inventory chart...\")\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    \n",
    "    excess_by_abc = sku_stats[sku_stats['StockStatus'] == 'Overstocked'].groupby('ABC_Class')['ExcessValue'].sum()\n",
    "    excess_by_abc = excess_by_abc.reindex(['A', 'B', 'C']).fillna(0)\n",
    "    \n",
    "    colors_excess = {'A': '#FF6B6B', 'B': '#FFA726', 'C': '#66BB6A'}\n",
    "    bar_colors = [colors_excess.get(cls, '#999999') for cls in excess_by_abc.index]\n",
    "    \n",
    "    bars = ax7.bar(excess_by_abc.index, excess_by_abc.values, color=bar_colors, alpha=0.8)\n",
    "    ax7.set_xlabel('ABC Class', fontsize=10)\n",
    "    ax7.set_ylabel('Excess Inventory Value ($)', fontsize=10)\n",
    "    ax7.set_title('Excess Inventory Value by ABC Class', fontsize=12, fontweight='bold')\n",
    "    ax7.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "    \n",
    "    # 8. INVENTORY TURNS\n",
    "    print(\"Creating inventory turns chart...\")\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    \n",
    "    turns_by_abc = sku_stats.groupby('ABC_Class')['AnnualTurns'].mean()\n",
    "    turns_by_abc = turns_by_abc.reindex(['A', 'B', 'C'])\n",
    "    \n",
    "    ax8.bar(turns_by_abc.index, turns_by_abc.values, \n",
    "            color=['#FFD700', '#C0C0C0', '#CD7F32'], alpha=0.8)\n",
    "    ax8.set_xlabel('ABC Class', fontsize=10)\n",
    "    ax8.set_ylabel('Average Annual Inventory Turns', fontsize=10)\n",
    "    ax8.set_title('Inventory Turnover by ABC Class', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 9. LEAD TIME DISTRIBUTION\n",
    "    print(\"Creating lead time distribution chart...\")\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    \n",
    "    leadtime_data = [sku_stats[sku_stats['ABC_Class'] == cls]['LeadTimeDays'] \n",
    "                     for cls in ['A', 'B', 'C']]\n",
    "    \n",
    "    box = ax9.boxplot(leadtime_data, labels=['A', 'B', 'C'], patch_artist=True)\n",
    "    \n",
    "    colors_box = ['#FFD700', '#C0C0C0', '#CD7F32']\n",
    "    for patch, color in zip(box['boxes'], colors_box):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax9.set_xlabel('ABC Class', fontsize=10)\n",
    "    ax9.set_ylabel('Lead Time (Days)', fontsize=10)\n",
    "    ax9.set_title('Lead Time Distribution by ABC Class', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f'{output_dir}/inventory_analysis_{timestamp}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualizations saved as: {filename}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Execute visualization\n",
    "fig = create_visualizations(sku_stats, abc_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results\n",
    "\n",
    "Export all results to Excel and CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(sku_stats, summary_stats, abc_summary):\n",
    "    \"\"\"\n",
    "    Export all results to Excel and CSV files for further analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üì§ EXPORTING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # EXPORT TO EXCEL\n",
    "    excel_filename = f'{output_dir}/inventory_optimization_results_{timestamp}.xlsx'\n",
    "    \n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "        # Sheet 1: SKU-Level Analysis\n",
    "        print(\"Creating SKU-level analysis sheet...\")\n",
    "        sku_export = sku_stats[[\n",
    "            'StockCode', 'Description', 'ABC_Class', \n",
    "            'ObsolescenceRisk', 'ObsolescenceType', 'DemandStability',\n",
    "            'AvgDailyDemand', 'DemandStd', 'DemandCV', 'DaysWithSales',\n",
    "            'AvgUnitPrice', 'TotalRevenue',\n",
    "            'LeadTimeDays', 'LeadTimeStd',\n",
    "            'ServiceLevelTarget', 'SafetyStock', 'ReorderPoint',\n",
    "            'CurrentStock', 'DaysOnHand', 'AnnualTurns',\n",
    "            'ServiceLevelAchievable', 'StockoutRisk', 'ServiceLevelGap',\n",
    "            'StockStatus', 'StatusReason',\n",
    "            'ExcessStock', 'DeficitStock', 'ExcessValue', 'DeficitValue',\n",
    "            'InventoryValue', 'DailyHoldingCost', 'ExcessHoldingCost'\n",
    "        ]].copy()\n",
    "        \n",
    "        sku_export['Priority'] = sku_export['ABC_Class'].map({'A': 1, 'B': 2, 'C': 3})\n",
    "        sku_export = sku_export.sort_values(['Priority', 'ExcessValue'], ascending=[True, False])\n",
    "        sku_export = sku_export.drop('Priority', axis=1)\n",
    "        \n",
    "        sku_export.to_excel(writer, sheet_name='SKU_Analysis', index=False)\n",
    "        \n",
    "        # Sheet 2: Summary Statistics\n",
    "        print(\"Creating summary statistics sheet...\")\n",
    "        summary_df = pd.DataFrame([\n",
    "            ['Total SKUs Analyzed', len(sku_stats)],\n",
    "            ['Total Revenue', f\"${sku_stats['TotalRevenue'].sum():,.2f}\"],\n",
    "            ['Total Inventory Value', f\"${sku_stats['InventoryValue'].sum():,.2f}\"],\n",
    "            ['Total Excess Value', f\"${sku_stats['ExcessValue'].sum():,.2f}\"],\n",
    "            ['Total Deficit Value', f\"${sku_stats['DeficitValue'].sum():,.2f}\"],\n",
    "            ['Potential Annual Savings', f\"${sku_stats['ExcessHoldingCost'].sum() * 365:,.2f}\"],\n",
    "            ['Data Retention', f\"{summary_stats['data_retention_pct']:.1f}%\"],\n",
    "            ['Time Period', f\"{summary_stats['time_range_start'].date()} to {summary_stats['time_range_end'].date()}\"],\n",
    "            ['ABC Class - A Items', f\"{abc_summary[abc_summary['ABC_Class']=='A']['StockCode'].values[0]:,} SKUs ({abc_summary[abc_summary['ABC_Class']=='A']['Pct_Revenue'].values[0]:.1f}% revenue)\"],\n",
    "            ['ABC Class - B Items', f\"{abc_summary[abc_summary['ABC_Class']=='B']['StockCode'].values[0]:,} SKUs ({abc_summary[abc_summary['ABC_Class']=='B']['Pct_Revenue'].values[0]:.1f}% revenue)\"],\n",
    "            ['ABC Class - C Items', f\"{abc_summary[abc_summary['ABC_Class']=='C']['StockCode'].values[0]:,} SKUs ({abc_summary[abc_summary['ABC_Class']=='C']['Pct_Revenue'].values[0]:.1f}% revenue)\"],\n",
    "            ['Stock Status - Optimal', f\"{(sku_stats['StockStatus']=='Optimal').sum():,} SKUs ({(sku_stats['StockStatus']=='Optimal').sum()/len(sku_stats)*100:.1f}%)\"],\n",
    "            ['Stock Status - Overstocked', f\"{(sku_stats['StockStatus']=='Overstocked').sum():,} SKUs ({(sku_stats['StockStatus']=='Overstocked').sum()/len(sku_stats)*100:.1f}%)\"],\n",
    "            ['Stock Status - Understocked', f\"{(sku_stats['StockStatus']=='Understocked').sum():,} SKUs ({(sku_stats['StockStatus']=='Understocked').sum()/len(sku_stats)*100:.1f}%)\"]\n",
    "        ], columns=['Metric', 'Value'])\n",
    "        \n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Sheet 3: ABC Class Summary\n",
    "        print(\"Creating ABC summary sheet...\")\n",
    "        abc_summary.to_excel(writer, sheet_name='ABC_Summary', index=False)\n",
    "        \n",
    "        # Sheet 4: Recommendations\n",
    "        print(\"Creating recommendations sheet...\")\n",
    "        recommendations = sku_stats[sku_stats['StockStatus'].isin(['Overstocked', 'Understocked'])].copy()\n",
    "        \n",
    "        def generate_recommendation(row):\n",
    "            if row['StockStatus'] == 'Overstocked':\n",
    "                action = \"Reduce stock\"\n",
    "                target = f\"Aim for: {row['ReorderPoint']:.0f} units\"\n",
    "                reason = f\"Currently {row['CurrentStock']:.0f} units ({row['ExcessStock']:.0f} excess)\"\n",
    "            else:\n",
    "                action = \"Increase stock\"\n",
    "                target = f\"Aim for: {row['ReorderPoint']:.0f} units\"\n",
    "                reason = f\"Currently {row['CurrentStock']:.0f} units ({row['DeficitStock']:.0f} deficit)\"\n",
    "            \n",
    "            priority = {'A': 'High', 'B': 'Medium', 'C': 'Low'}.get(row['ABC_Class'], 'Medium')\n",
    "            \n",
    "            return pd.Series({\n",
    "                'Priority': priority,\n",
    "                'Action': action,\n",
    "                'Target': target,\n",
    "                'Reason': reason,\n",
    "                'Impact': f\"${row['ExcessValue'] + row['DeficitValue']:,.2f}\"\n",
    "            })\n",
    "        \n",
    "        rec_details = recommendations.apply(generate_recommendation, axis=1)\n",
    "        recommendations = pd.concat([recommendations[['StockCode', 'Description', 'ABC_Class']], \n",
    "                                    rec_details], axis=1)\n",
    "        \n",
    "        recommendations = recommendations.sort_values(['Priority', 'Impact'], ascending=[True, False])\n",
    "        recommendations.to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Excel file exported: {excel_filename}\")\n",
    "    \n",
    "    # EXPORT TO CSV (FOR POWER BI)\n",
    "    csv_filename = f'{output_dir}/inventory_data_for_powerbi_{timestamp}.csv'\n",
    "    \n",
    "    powerbi_data = sku_stats[[\n",
    "        'StockCode', 'Description', 'ABC_Class', 'ObsolescenceRisk',\n",
    "        'AvgDailyDemand', 'DemandStd', 'DemandCV',\n",
    "        'LeadTimeDays', 'SafetyStock', 'ReorderPoint',\n",
    "        'CurrentStock', 'StockStatus',\n",
    "        'ExcessStock', 'DeficitStock', 'ExcessValue', 'DeficitValue',\n",
    "        'InventoryValue', 'ServiceLevelTarget', 'ServiceLevelAchievable',\n",
    "        'AnnualTurns', 'DaysOnHand'\n",
    "    ]].copy()\n",
    "    \n",
    "    powerbi_data['StatusColor'] = powerbi_data['StockStatus'].map({\n",
    "        'Optimal': '#4ECDC4',\n",
    "        'Overstocked': '#FF6B6B',\n",
    "        'Understocked': '#45B7D1'\n",
    "    })\n",
    "    \n",
    "    powerbi_data['ActionPriority'] = powerbi_data.apply(\n",
    "        lambda x: 'Immediate' if x['ABC_Class'] == 'A' and x['StockStatus'] != 'Optimal'\n",
    "        else 'High' if x['ABC_Class'] == 'B' and x['StockStatus'] != 'Optimal'\n",
    "        else 'Medium' if x['StockStatus'] != 'Optimal'\n",
    "        else 'Low', axis=1\n",
    "    )\n",
    "    \n",
    "    powerbi_data.to_csv(csv_filename, index=False)\n",
    "    print(f\"‚úÖ CSV file for Power BI exported: {csv_filename}\")\n",
    "    \n",
    "    return excel_filename, csv_filename\n",
    "\n",
    "# Execute export\n",
    "excel_file, csv_file = export_results(sku_stats, summary_stats, abc_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Report\n",
    "\n",
    "Display key performance indicators and actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ PROJECT COMPLETION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate key metrics\n",
    "total_skus = len(sku_stats)\n",
    "overstocked_skus = (sku_stats['StockStatus'] == 'Overstocked').sum()\n",
    "understocked_skus = (sku_stats['StockStatus'] == 'Understocked').sum()\n",
    "optimal_skus = (sku_stats['StockStatus'] == 'Optimal').sum()\n",
    "\n",
    "total_excess_value = sku_stats['ExcessValue'].sum()\n",
    "total_deficit_value = sku_stats['DeficitValue'].sum()\n",
    "total_inventory_value = sku_stats['InventoryValue'].sum()\n",
    "\n",
    "annual_savings = sku_stats['ExcessHoldingCost'].sum() * 365\n",
    "\n",
    "# A-items performance\n",
    "a_items = sku_stats[sku_stats['ABC_Class'] == 'A']\n",
    "a_optimal_pct = (a_items['StockStatus'] == 'Optimal').sum() / len(a_items) * 100 if len(a_items) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä KEY PERFORMANCE INDICATORS:\")\n",
    "print(f\"  1. Inventory Health: {optimal_skus/total_skus*100:.1f}% SKUs optimally stocked\")\n",
    "print(f\"  2. A-Item Performance: {a_optimal_pct:.1f}% of A-items optimally stocked\")\n",
    "print(f\"  3. Excess Inventory: ${total_excess_value:,.2f} ({total_excess_value/total_inventory_value*100:.1f}% of total)\")\n",
    "print(f\"  4. Service Level Gap: {(sku_stats['ServiceLevelGap'].mean()*100):.1f}% average gap\")\n",
    "print(f\"  5. Potential Annual Savings: ${annual_savings:,.2f}\")\n",
    "\n",
    "print(f\"\\nüìà ACTIONABLE INSIGHTS:\")\n",
    "\n",
    "# Insight 1: Top overstocked A-items\n",
    "top_overstocked_a = sku_stats[(sku_stats['ABC_Class'] == 'A') & \n",
    "                             (sku_stats['StockStatus'] == 'Overstocked')]\n",
    "if len(top_overstocked_a) > 0:\n",
    "    top_overstocked_a = top_overstocked_a.nlargest(3, 'ExcessValue')\n",
    "    print(f\"  1. Top 3 Overstocked A-items (High Priority):\")\n",
    "    for idx, row in top_overstocked_a.iterrows():\n",
    "        print(f\"     ‚Ä¢ {row['StockCode']}: {row['ExcessStock']:.0f} units excess (${row['ExcessValue']:.2f})\")\n",
    "\n",
    "# Insight 2: Understocked high-service items\n",
    "understocked_high_service = sku_stats[(sku_stats['StockStatus'] == 'Understocked') & \n",
    "                                     (sku_stats['ServiceLevelTarget'] > 0.95)]\n",
    "if len(understocked_high_service) > 0:\n",
    "    print(f\"  2. High-Service Items Understocked:\")\n",
    "    for idx, row in understocked_high_service.head(3).iterrows():\n",
    "        print(f\"     ‚Ä¢ {row['StockCode']}: Service gap {row['ServiceLevelGap']*100:.1f}%\")\n",
    "\n",
    "# Insight 3: Seasonal obsolescence risk\n",
    "seasonal_high_risk = sku_stats[(sku_stats['ObsolescenceRisk'] == 'High') & \n",
    "                              (sku_stats['ObsolescenceType'] == 'Seasonal')]\n",
    "if len(seasonal_high_risk) > 0:\n",
    "    print(f\"  3. Seasonal Items with High Obsolescence Risk:\")\n",
    "    print(f\"     ‚Ä¢ {len(seasonal_high_risk)} seasonal items identified\")\n",
    "    print(f\"     ‚Ä¢ Consider implementing clearance strategies\")\n",
    "\n",
    "print(f\"\\nüìÅ OUTPUT FILES:\")\n",
    "print(f\"  ‚Ä¢ {excel_file} - Complete analysis with multiple sheets\")\n",
    "print(f\"  ‚Ä¢ {csv_file} - Optimized for Power BI dashboard\")\n",
    "\n",
    "print(f\"\\n‚úÖ PROJECT COMPLETED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Recommendations\n",
    "\n",
    "View top 5 recommendations for A-items with issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° SAMPLE RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get top 5 recommendations (A items with issues)\n",
    "recommendations = sku_stats[sku_stats['ABC_Class'] == 'A']\n",
    "recommendations = recommendations[recommendations['StockStatus'] != 'Optimal']\n",
    "\n",
    "if len(recommendations) > 0:\n",
    "    recommendations = recommendations.nlargest(5, 'ExcessValue')\n",
    "    \n",
    "    for idx, row in recommendations.iterrows():\n",
    "        print(f\"\\nüîπ {row['StockCode']} - {row['Description'][:50]}...\")\n",
    "        print(f\"   Status: {row['StockStatus']}\")\n",
    "        print(f\"   Current: {row['CurrentStock']:.0f} units\")\n",
    "        print(f\"   Target: {row['ReorderPoint']:.0f} units\")\n",
    "        \n",
    "        if row['StockStatus'] == 'Overstocked':\n",
    "            print(f\"   Excess: {row['ExcessStock']:.0f} units (${row['ExcessValue']:.2f})\")\n",
    "            print(f\"   Action: Reduce inventory by {row['ExcessStock']:.0f} units\")\n",
    "        else:\n",
    "            print(f\"   Deficit: {row['DeficitStock']:.0f} units (${row['DeficitValue']:.2f})\")\n",
    "            print(f\"   Action: Increase inventory by {row['DeficitStock']:.0f} units\")\n",
    "        \n",
    "        print(f\"   Impact: ${row['ExcessValue'] + row['DeficitValue']:.2f}\")\n",
    "else:\n",
    "    print(\"All A-items are optimally stocked! Great inventory management.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}